# -*- coding: utf-8 -*-
"""Ainstrument-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LsSVI3nGzHMAL283shrZoW5WcigQQHr_
"""

print('Installing from pip package...')
!pip install flask-ngrok
!pip install -U flask-cors
!pip install pyrebase
!pip install librosa==0.7
!pip install -qU ddsp==1.0.1
# !pip install --upgrade tensorflow

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow.compat.v2 as tf
import copy
import os
import time
import crepe
import ddsp
import ddsp.training
import gin
from ddsp.colab import colab_utils
import librosa
import numpy as np
import pickle
import tensorflow_datasets as tfds
from scipy.io import wavfile
from tqdm import tqdm
import warnings
from google.colab import files
warnings.filterwarnings("ignore")
import time
from flask import Flask, request, jsonify, Response, send_file, make_response
from flask_cors import CORS
from flask_ngrok import run_with_ngrok
import json
import pyrebase
import requests
import librosa
import json
from ddsp.colab.colab_utils import (
    auto_tune, detect_notes, fit_quantile_transform, 
    get_tuning_factor, download, play, record, 
    specplot, upload, DEFAULT_SAMPLE_RATE, audio_bytes_to_np)
import io

DEFAULT_SAMPLE_RATE

class MusicConversion:
    def __init__(self, songName, model, songPath):
        self.songName = songName
        self.songPath = ''
        self.extractedDestinationPath = ''
        self.model = model
        try:
          os.makedirs('/content/outputSongs')
        except:
          print("out exist")

    def loadSongandExtractFeatures(self):
        # self.songPath = 'songPath'
        # self.songPath = '/content/extractedVocals/kabira.wav'
        # files.download(self.songPath)
        
        filenames, audios = upload()
        audio = audios[0][0]
        audio = audio[np.newaxis, :]
        # play(audio)
        ddsp.spectral_ops.reset_crepe()
        print("Extracting Features")
        start_time = time.time()
        
        audio_features = ddsp.training.metrics.compute_audio_features(audio)
        audio_features['loudness_db'] = audio_features['loudness_db'].astype(np.float32)
        audio_features_mod = None
        print('Audio features took %.1f seconds' % (time.time() - start_time))
        
        model = self.model
        if model in ('Violin', 'Flute', 'Flute2', 'Trumpet', 'Tenor_Saxophone'):
          print(model, " model here i m")
          PRETRAINED_DIR = '/content/pretrained'
          # Copy over from gs:// for faster loading.
          !rm -r $PRETRAINED_DIR &> /dev/null
          !mkdir $PRETRAINED_DIR &> /dev/null
          GCS_CKPT_DIR = 'gs://ddsp/models/timbre_transfer_colab/2021-01-06'
          model_dir = os.path.join(GCS_CKPT_DIR, 'solo_%s_ckpt' % model.lower())
          
          !gsutil cp $model_dir/* $PRETRAINED_DIR &> /dev/null
          model_dir = PRETRAINED_DIR
          gin_file = os.path.join(model_dir, 'operative_config-0.gin')
        
        DATASET_STATS = None
        dataset_stats_file = os.path.join(model_dir, 'dataset_statistics.pkl')
        print(f'Loading dataset statistics from {dataset_stats_file}')
        try:
          if tf.io.gfile.exists(dataset_stats_file):
            with tf.io.gfile.GFile(dataset_stats_file, 'rb') as f:
              DATASET_STATS = pickle.load(f)
        except Exception as err:
          print('Loading dataset statistics from pickle failed: {}.'.format(err))
        
        with gin.unlock_config():
          gin.parse_config_file(gin_file, skip_unknown=True)

        # Assumes only one checkpoint in the folder, 'ckpt-[iter]`.
        ckpt_files = [f for f in tf.io.gfile.listdir(model_dir) if 'ckpt' in f]
        ckpt_name = ckpt_files[0].split('.')[0]
        ckpt = os.path.join(model_dir, ckpt_name)

        # Ensure dimensions and sampling rates are equal
        time_steps_train = gin.query_parameter('F0LoudnessPreprocessor.time_steps')
        n_samples_train = gin.query_parameter('Harmonic.n_samples')
        hop_size = int(n_samples_train / time_steps_train)

        time_steps = int(audio.shape[1] / hop_size)
        n_samples = time_steps * hop_size

        gin_params = [
          'Harmonic.n_samples = {}'.format(n_samples),
          'FilteredNoise.n_samples = {}'.format(n_samples),
          'F0LoudnessPreprocessor.time_steps = {}'.format(time_steps),
          'oscillator_bank.use_angular_cumsum = True',  # Avoids cumsum accumulation errors.
        ]

        with gin.unlock_config():
          gin.parse_config(gin_params)
        
        for key in ['f0_hz', 'f0_confidence', 'loudness_db']:
          audio_features[key] = audio_features[key][:time_steps]
        
        audio_features['audio'] = audio_features['audio'][:, :n_samples]
        model = ddsp.training.models.Autoencoder()
        model.restore(ckpt)

        start_time = time.time()
        _ = model(audio_features, training=False)
        print('Restoring model took %.1f seconds' % (time.time() - start_time))

        threshold = 1
        ADJUST = True
        quiet = 20
        autotune = 0
        pitch_shift =  0
        loudness_shift = 0
        audio_features_mod = {k: v.copy() for k, v in audio_features.items()}
        mask_on = None

        if ADJUST and DATASET_STATS is not None:
          # Detect sections that are "on".
          mask_on, note_on_value = detect_notes(audio_features['loudness_db'],
                                        audio_features['f0_confidence'],
                                        threshold)

          if np.any(mask_on):
            # Shift the pitch register.
            target_mean_pitch = DATASET_STATS['mean_pitch']
            pitch = ddsp.core.hz_to_midi(audio_features['f0_hz'])
            mean_pitch = np.mean(pitch[mask_on])
            p_diff = target_mean_pitch - mean_pitch
            p_diff_octave = p_diff / 12.0
            round_fn = np.floor if p_diff_octave > 1.5 else np.ceil
            p_diff_octave = round_fn(p_diff_octave)
            audio_features_mod = self.shift_f0(audio_features_mod, p_diff_octave)


            # Quantile shift the note_on parts.
            _, loudness_norm = colab_utils.fit_quantile_transform(
                audio_features['loudness_db'],
                mask_on,
                inv_quantile=DATASET_STATS['quantile_transform'])

            # Turn down the note_off parts.
            mask_off = np.logical_not(mask_on)
            loudness_norm[mask_off] -=  quiet * (1.0 - note_on_value[mask_off][:, np.newaxis])
            loudness_norm = np.reshape(loudness_norm, audio_features['loudness_db'].shape)
            
            audio_features_mod['loudness_db'] = loudness_norm 

            # Auto-tune.
            if autotune:
              f0_midi = np.array(ddsp.core.hz_to_midi(audio_features_mod['f0_hz']))
              tuning_factor = get_tuning_factor(f0_midi, audio_features_mod['f0_confidence'], mask_on)
              f0_midi_at = auto_tune(f0_midi, tuning_factor, mask_on, amount=autotune)
              audio_features_mod['f0_hz'] = ddsp.core.midi_to_hz(f0_midi_at)

          else:
            print('\nSkipping auto-adjust (no notes detected or ADJUST box empty).')
        
        else:
          print('\nSkipping auto-adujst (box not checked or no dataset statistics found).')

        audio_features_mod = self.shift_ld(audio_features_mod, loudness_shift)
        audio_features_mod = self.shift_f0(audio_features_mod, pitch_shift)

        af = audio_features if audio_features_mod is None else audio_features_mod

        # Run a batch of predictions.
        start_time = time.time()
        outputs = model(af, training=False)
        audio_gen = model.get_audio_from_outputs(outputs)
        print('Prediction took %.1f seconds' % (time.time() - start_time))
        uploadTime = str(time.time())
        path = '/content/outputSongs/' + uploadTime + "/"+self.songName+".wav"
        os.makedirs('/content/outputSongs/' + uploadTime)
        print(type(audio_gen))
        librosa.output.write_wav(path, np.ravel(audio_gen), sr=DEFAULT_SAMPLE_RATE)
        # wavfile.write(path, DEFAULT_SAMPLE_RATE, audio_gen)
        # play(audio_gen)
        print(path)
        return path

    def shift_ld(self, audio_features, ld_shift=0.0):
      """Shift loudness by a number of ocatves."""
      audio_features['loudness_db'] += ld_shift
      return audio_features


    def shift_f0(self, audio_features, pitch_shift=0.0):
      """Shift f0 by a number of ocatves."""
      audio_features['f0_hz'] *= 2.0 ** (pitch_shift)
      audio_features['f0_hz'] = np.clip(audio_features['f0_hz'], 
                                        0.0, 
                                        librosa.midi_to_hz(110.0))
      return audio_features

        
    
# def main():
#     obj = MusicConversion("songName", 'Violin', "")
#     obj.loadSongandExtractFeatures()

# if __name__ == "__main__":
#     main()

app = Flask(__name__)
CORS(app)

firebaseConfig = {
    "apiKey": "AIzaSyAcbBkzH2YnTPVyDhKGjeA7EFAQf3wNTeE",
    "authDomain": "ainstrument-a03f0.firebaseapp.com",
    "databaseURL": "https://ainstrument-a03f0-default-rtdb.firebaseio.com",
    "storageBucket": "ainstrument-a03f0.appspot.com",
}

try:
  upload_dir = '/content/uploadedSongs'
  os.makedirs(upload_dir)
except:
  print("upload_dir exixts")

@app.route('/convertSong', methods=['POST'])
def convertSong():
  song = request.files['song']
  songName = request.form['songName']
  instrument = request.form['instrument']
  print(instrument)
  idToken = request.form['idToken']
  userUID = request.form['userUID']
  # print('idToken ',idToken)
  try:
    uploadTime = time.time()
    songPath = upload_dir+"/"+str(uploadTime)+"/"+song.filename
    os.makedirs(upload_dir+"/"+str(uploadTime))
    song.save(songPath)
    
    storage = firebase.storage()
    res = storage.child("uploadedSongs/"+str(uploadTime)+"/"+song.filename).put(songPath, idToken)
    print("oh yesss ",res)
    uploadedUrl = storage.child("uploadedSongs/"+str(uploadTime)+"/"+song.filename).get_url(idToken)
    print(uploadedUrl)
    obj = MusicConversion(songName, instrument, songPath)
    path = obj.loadSongandExtractFeatures()

    res = storage.child("extractedSongs/"+str(uploadTime)+"/"+song.filename).put(path, idToken)
    extractedUrl = storage.child("extractedSongs/"+str(uploadTime)+"/"+song.filename).get_url(idToken)

    db = firebase.database()
    count = db.child("users").child(userUID).child("songs").shallow().get(idToken)
    if count.val() != None:    
      _ = db.child("users").child(userUID).child("songs").child(str(len(count.val()))).update({"songName":song.filename,"extractedSongUrl":extractedUrl,"uploadedSongUrl":uploadedUrl, "thumbnailUrl":"", "instrument":instrument}, idToken)
    else:
      _ = db.child("users").child(userUID).child("songs").child("0").update({"songName":song.filename,"extractedSongUrl": extractedUrl, "uploadedSongUrl": uploadedUrl, "thumbnailUrl":"", "instrument":instrument}, idToken)
    
    return jsonify({"success":1, "message":"Extraction and saved succesfully","data":{"extractedSongUrl": extractedUrl, "uploadedSongUrl": uploadedUrl}}), 200

  
  
  except requests.exceptions.HTTPError as error:
    error = json.loads(error.args[1])
    errorMessage = error.get("error").get("message")
    statusCode = error.get("error").get("code")
    return make_response(
              jsonify(
                  {
                      "message":errorMessage,
                      "success":0
                  }
              ), 200
          )
  
  except:
    return jsonify({"success":0, "message":"Error please try again later..."}), 200

if __name__ == "__main__":
    firebase = pyrebase.initialize_app(firebaseConfig)  
    run_with_ngrok(app)
    app.run()

# db = firebase.database()
# users = db.child("users").child("OX081QI702XwdHMRJ1Q64T7PjeC2").get(idToken)
# print(users.val())

# try:
#   s = db.child("users").child("OX0802XwdHMRJ1Q64TPjeC2").child("songs").shallow().get()
# except requests.exceptions.HTTPError as error:
#   error = json.loads(error.args[1])
#   print(error.get('error'))

# librosa.output.write_wav('/content/outputSongs/audio_gen.wav', np.ravel(s), sr=16000)

# db = firebase.database()
# idToken = "eyJhbGciOiJSUzI1NiIsImtpZCI6IjYxMDgzMDRiYWRmNDc1MWIyMWUwNDQwNTQyMDZhNDFkOGZmMWNiYTgiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL3NlY3VyZXRva2VuLmdvb2dsZS5jb20vYWluc3RydW1lbnQtYTAzZjAiLCJhdWQiOiJhaW5zdHJ1bWVudC1hMDNmMCIsImF1dGhfdGltZSI6MTYxMzYzMzQ0MSwidXNlcl9pZCI6IkY4ejNBU1Fvc2djbW9ObDBNQ1BIb1FXbXIwSTMiLCJzdWIiOiJGOHozQVNRb3NnY21vTmwwTUNQSG9RV21yMEkzIiwiaWF0IjoxNjEzNjMzNDQxLCJleHAiOjE2MTM2MzcwNDEsImVtYWlsIjoiYmhhdmVzaHNpbmdoNTc5QGdtYWlsLmNvbSIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJmaXJlYmFzZSI6eyJpZGVudGl0aWVzIjp7ImVtYWlsIjpbImJoYXZlc2hzaW5naDU3OUBnbWFpbC5jb20iXX0sInNpZ25faW5fcHJvdmlkZXIiOiJwYXNzd29yZCJ9fQ.Q-gulZ1CSWAvJLGlJXy906k9BmRutTZch7TVbJmCqSTMEaw7THsDgM68lnfeA-TyAv0lijqefcolafkLY6pXgXwq2_fHRky_zpLAq4EuGWYhbYbzuhJRbJRa4W4eiObJJMSUkOdqRL8L76q6CL8K92xosK3RoQ_9JBRh1e8tCg1Y6bWmnsxytXiAZ_N6Lr8l4g-0AqFM25DUPNxCvkb9PtENpwGMD9A2NtHNb24K34ljWkhI61YX5dwegsTCZ132wcRwHu43K4wQbGxnNc92P4-2XpMN7MidorvxFgh5s4-BfNXET5MxdCUxCxil1Dp6dVJTRz9toxwdG35tWP5zqg"
# count = db.child("users").child("F8z3ASQosgcmoNl0MCPHoQWmr0I3").child("songs").shallow().get(idToken)
# if count.val() != None:
#   _ = db.child("users").child("F8z3ASQosgcmoNl0MCPHoQWmr0I3").child("songs").child(str(len(count.val()))).update({"extracted":"ok2","de":"ok2"}, idToken)
# else:
#   _ = db.child("users").child("F8z3ASQosgcmoNl0MCPHoQWmr0I3").child("songs").child("0").update({"extracted":"ok2","de":"ok2"}, idToken)

# uploadedUrl = storage.child("uploadedSongs/"+str(1613586600.3376124)+"/"+"kabira.wav").get_url(idToken)

!pip install pysndfx

from pysndfx import AudioEffectsChain

fx = (
    AudioEffectsChain()
    .highshelf()
    .reverb()
    .phaser()
    .delay()
    .lowshelf()
)

infile = '/content/outputSongs/1617548452.1110184/test.wav'
outfile = 'my_processed_audio_file.ogg'
fx(infile, outfile)
# # Apply phaser and reverb directly to an audio file.
# from librosa import load
# y, sr = load(infile, sr=None)
# y = fx(y)

# # Apply the effects and return the results as a ndarray.
# y = fx(infile)

# # Apply the effects to a ndarray but store the resulting audio to disk.
# fx(x, outfile)

!apt install sox

